{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2a9174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.891613111148901\n",
      "Epoch 100 loss: 1.2797301119568272\n",
      "Epoch 200 loss: 0.6916709524245601\n",
      "Epoch 300 loss: 0.5274013672010436\n",
      "Epoch 400 loss: 0.4530128366518832\n",
      "Epoch 500 loss: 0.4110486750205873\n",
      "Epoch 600 loss: 0.38415602912191843\n",
      "Epoch 700 loss: 0.3653160188675642\n",
      "Epoch 800 loss: 0.3512372700156132\n",
      "Epoch 900 loss: 0.34020773833015105\n",
      "Epoch 1000 loss: 0.3312562002257787\n",
      "Epoch 1100 loss: 0.3237893764904585\n",
      "Epoch 1200 loss: 0.3174219398266385\n",
      "Epoch 1300 loss: 0.31189096884447237\n",
      "Epoch 1400 loss: 0.30700990025713365\n",
      "Epoch 1500 loss: 0.3026421531335177\n",
      "Epoch 1600 loss: 0.29868516609817486\n",
      "Epoch 1700 loss: 0.29506027391024614\n",
      "Epoch 1800 loss: 0.2917060351162225\n",
      "Epoch 1900 loss: 0.28857369479551503\n",
      "Train accuracy: 0.9177857142857143\n",
      "Test accuracy: 0.9165\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Fetch the MNIST dataset\n",
    "mnist = fetch_openml(\"mnist_784\")\n",
    "\n",
    "# Assign data to X and targets to y, both converted to appropriate types\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "# Normalize pixel values\n",
    "X /= 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases for the hidden and output layers\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass through the network\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        exp_scores = np.exp(self.z2)\n",
    "        self.probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        # Compute the loss\n",
    "        probs = self.forward(X)\n",
    "        log_probs = -np.log(probs[range(len(y)), y])\n",
    "        return np.mean(log_probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the class labels\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        # Train the network\n",
    "        for epoch in range(epochs):\n",
    "            probs = self.forward(X)\n",
    "            delta = probs.copy()\n",
    "            delta[range(len(y)), y] -= 1\n",
    "            delta /= len(y)\n",
    "\n",
    "            # Backpropagation\n",
    "            dW2 = np.dot(self.a1.T, delta)\n",
    "            db2 = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "            delta2 = np.dot(delta, self.W2.T) * (1 - np.power(self.a1, 2))\n",
    "            dW1 = np.dot(X.T, delta2)\n",
    "            db1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "            # Gradient descent\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "\n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} loss: {self.loss(X, y)}\")\n",
    "\n",
    "# Set the hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 512\n",
    "output_size = 50\n",
    "learning_rate = 0.05\n",
    "epochs = 2000\n",
    "\n",
    "# Create and train the MLP\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "mlp.train(X_train, y_train, epochs, learning_rate)\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = mlp.predict(X_train)\n",
    "train_accuracy = np.mean(y_pred_train == y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccebef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with hidden layer size 32\n",
      "Epoch 0 loss: 2.30253198423809\n",
      "Epoch 100 loss: 2.076991802790045\n",
      "Epoch 200 loss: 1.237112446417689\n",
      "Epoch 300 loss: 0.8374995897909436\n",
      "Epoch 400 loss: 0.6462378090234638\n",
      "Epoch 500 loss: 0.54197243122276\n",
      "Epoch 600 loss: 0.4769855814464789\n",
      "Epoch 700 loss: 0.4331372083463388\n",
      "Epoch 800 loss: 0.4018873941514794\n",
      "Epoch 900 loss: 0.3785301381231243\n",
      "Epoch 1000 loss: 0.36031290290749685\n",
      "Epoch 1100 loss: 0.34557206476546276\n",
      "Epoch 1200 loss: 0.3332704078013054\n",
      "Epoch 1300 loss: 0.3227400941130983\n",
      "Epoch 1400 loss: 0.3135369708439419\n",
      "Epoch 1500 loss: 0.3053563030525885\n",
      "Epoch 1600 loss: 0.2979828935146342\n",
      "Epoch 1700 loss: 0.2912607769269981\n",
      "Epoch 1800 loss: 0.2850742776754602\n",
      "Epoch 1900 loss: 0.27933581462482504\n",
      "Train accuracy: 0.9229821428571429\n",
      "Train confusion matrix:\n",
      "[[5388    0   30   13   12   29   35    6   43    4]\n",
      " [   0 6096   29   21    9   17   10   17   70    8]\n",
      " [  52   46 5059   58   77    8  101   68  128   13]\n",
      " [  21   27  113 5108    3  193   22   71   98   52]\n",
      " [  10   30   27    3 5175    5   56    6   24  193]\n",
      " [  78   27   29  175   58 4360   97   16  150   50]\n",
      " [  49   19   50    1   43   52 5235    0   31    0]\n",
      " [  30   43   77   19   70    8    1 5404   12  126]\n",
      " [  27  105   63  120   27  123   40   20 4877   66]\n",
      " [  46   27   16   75  184   37    5  130   33 4985]]\n",
      "Test accuracy: 0.9202142857142858\n",
      "Test confusion matrix:\n",
      "[[1299    0    6    1    4    9   12    3    9    0]\n",
      " [   0 1552    3   14    1    6    1    5   16    2]\n",
      " [  12   12 1241   14   21    2   28   19   29    2]\n",
      " [   4    5   26 1288    1   43    9   19   25   13]\n",
      " [   4    6    9    2 1197    0   12    2    3   60]\n",
      " [  15    7    3   57   14 1109   17    2   39   10]\n",
      " [   7    6   18    0   18   18 1324    0    5    0]\n",
      " [  11   11   22    3   11    3    0 1409    2   31]\n",
      " [   6   26   17   47    9   34   16    8 1179   15]\n",
      " [  12    9    7   15   41    6    0   34   11 1285]]\n",
      "\n",
      "Training MLP with hidden layer size 128\n",
      "Epoch 0 loss: 2.3009600798579246\n",
      "Epoch 100 loss: 1.6412431922737245\n",
      "Epoch 200 loss: 0.8713086385063211\n",
      "Epoch 300 loss: 0.6220091735420684\n",
      "Epoch 400 loss: 0.5116876404464099\n",
      "Epoch 500 loss: 0.4496143807896831\n",
      "Epoch 600 loss: 0.4104596918619195\n",
      "Epoch 700 loss: 0.3838085895365892\n",
      "Epoch 800 loss: 0.3644264235981628\n",
      "Epoch 900 loss: 0.349524841087281\n",
      "Epoch 1000 loss: 0.33755118040319054\n",
      "Epoch 1100 loss: 0.3275925076387582\n",
      "Epoch 1200 loss: 0.31908016960918023\n",
      "Epoch 1300 loss: 0.31164041555833566\n",
      "Epoch 1400 loss: 0.30501599662525875\n",
      "Epoch 1500 loss: 0.29902306817226143\n",
      "Epoch 1600 loss: 0.2935263094801139\n",
      "Epoch 1700 loss: 0.28842385137950044\n",
      "Epoch 1800 loss: 0.2836377203821748\n",
      "Epoch 1900 loss: 0.2791075223562682\n",
      "Train accuracy: 0.9217321428571429\n",
      "Train confusion matrix:\n",
      "[[5383    0   20   14    9   36   36    7   50    5]\n",
      " [   2 6090   33   18    8   28    8   18   63    9]\n",
      " [  44   43 5039   78   90   16   79   68  132   21]\n",
      " [  18   22  117 5103    3  203   23   66  102   51]\n",
      " [  10   27   28    4 5153    3   63    8   30  203]\n",
      " [  74   26   33  170   55 4372   95   18  143   54]\n",
      " [  38   19   44    2   47   61 5235    1   33    0]\n",
      " [  27   40   71   20   51    7    3 5393   11  167]\n",
      " [  28  108   56  114   23  138   45   13 4872   71]\n",
      " [  42   28   16   70  168   35    4  157   41 4977]]\n",
      "Test accuracy: 0.9199285714285714\n",
      "Test confusion matrix:\n",
      "[[1299    0    5    1    3    7   14    1   12    1]\n",
      " [   0 1547    4   13    1    9    1    5   18    2]\n",
      " [   7   15 1237   13   21    5   29   19   30    4]\n",
      " [   3    7   30 1285    1   44    9   18   22   14]\n",
      " [   4    4    8    3 1195    0   13    4    6   58]\n",
      " [  13    8    4   53   14 1113   14    4   40   10]\n",
      " [   3    5   15    0   18   23 1329    0    3    0]\n",
      " [   8    9   23    5   11    5    0 1407    2   33]\n",
      " [   7   25   10   45    7   36   17    8 1190   12]\n",
      " [   9   10    7   14   41    9    0   44    9 1277]]\n",
      "\n",
      "Training MLP with hidden layer size 1024\n",
      "Epoch 0 loss: 2.2965227645373663\n",
      "Epoch 100 loss: 0.9809450188251853\n",
      "Epoch 200 loss: 0.5915849862842574\n",
      "Epoch 300 loss: 0.47652214402453824\n",
      "Epoch 400 loss: 0.4224374665860678\n",
      "Epoch 500 loss: 0.39115235198774506\n",
      "Epoch 600 loss: 0.3705841740422512\n",
      "Epoch 700 loss: 0.3558509399810134\n",
      "Epoch 800 loss: 0.34465510262988464\n",
      "Epoch 900 loss: 0.33578029502938\n",
      "Epoch 1000 loss: 0.32851972897884635\n",
      "Epoch 1100 loss: 0.3224318197133864\n",
      "Epoch 1200 loss: 0.3172248072882203\n",
      "Epoch 1300 loss: 0.3126973938201616\n",
      "Epoch 1400 loss: 0.308705872090733\n",
      "Epoch 1500 loss: 0.3051447707643789\n",
      "Epoch 1600 loss: 0.30193487012548714\n",
      "Epoch 1700 loss: 0.29901547226345787\n",
      "Epoch 1800 loss: 0.29633924724549604\n",
      "Epoch 1900 loss: 0.293868702013664\n",
      "Train accuracy: 0.9171071428571429\n",
      "Train confusion matrix:\n",
      "[[5381    0   21   16    9   36   38    6   48    5]\n",
      " [   2 6091   31   16    5   30    7   16   71    8]\n",
      " [  39   50 4999   85   93   24   77   73  142   28]\n",
      " [  19   24  125 5076    3  214   22   66  100   59]\n",
      " [  11   27   33    4 5138    3   63    9   35  206]\n",
      " [  74   34   40  177   59 4336  107   21  140   52]\n",
      " [  43   20   45    2   49   60 5214    7   38    2]\n",
      " [  21   38   77   21   60    7    3 5358   13  192]\n",
      " [  34  114   64  129   23  144   42   21 4822   75]\n",
      " [  36   28   18   75  172   33    4  185   44 4943]]\n",
      "Test accuracy: 0.9148571428571428\n",
      "Test confusion matrix:\n",
      "[[1295    0    5    1    3   11   14    2   11    1]\n",
      " [   0 1540    5   12    1   10    1    4   24    3]\n",
      " [   6   14 1225   18   17    6   28   23   36    7]\n",
      " [   5    7   32 1278    1   46    9   18   22   15]\n",
      " [   4    4    7    5 1194    1   14    5    6   55]\n",
      " [  12    8    6   57   12 1107   15    3   43   10]\n",
      " [   4    5   12    1   16   21 1333    1    2    1]\n",
      " [   8   10   24    6   11    5    0 1398    2   39]\n",
      " [   8   28   13   52    7   37   17    7 1173   15]\n",
      " [   8   10    9   14   45    7    0   50   12 1265]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the input size based on the training data\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# There are 10 classes (0 to 9) in MNIST\n",
    "output_size = 10  \n",
    "\n",
    "# Define the learning rate and the number of training epochs\n",
    "learning_rate = 0.05\n",
    "epochs = 2000\n",
    "\n",
    "# Different values for hidden layer size that we want to test\n",
    "hidden_sizes = [32, 128, 1024]  \n",
    "\n",
    "# Loop over each hidden layer size\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nTraining MLP with hidden layer size {hidden_size}\")\n",
    "\n",
    "    # Initialize the MLP with the current hidden layer size\n",
    "    mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Train the MLP on the training data\n",
    "    mlp.train(X_train, y_train, epochs, learning_rate)\n",
    "\n",
    "    # Predict the labels for the training data\n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "\n",
    "    # Calculate and print the training accuracy\n",
    "    train_accuracy = np.mean(y_pred_train == y_train)\n",
    "    print(f\"Train accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Calculate and print the training confusion matrix\n",
    "    print(\"Train confusion matrix:\")\n",
    "    print(confusion_matrix(y_train, y_pred_train))\n",
    "\n",
    "    # Predict the labels for the test data\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "\n",
    "    # Calculate and print the test accuracy\n",
    "    test_accuracy = np.mean(y_pred_test == y_test)\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Calculate and print the test confusion matrix\n",
    "    print(\"Test confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f6be414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for k=3:\n",
      "Accuracy: 0.9861111111111112\n",
      "Confusion Matrix:\n",
      "[[36  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 34  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 37  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 36  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 36  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 35  0  1]\n",
      " [ 0  2  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "\n",
      "\n",
      "Results for k=5:\n",
      "Accuracy: 0.9916666666666667\n",
      "Confusion Matrix:\n",
      "[[36  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 34  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 37  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 36  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  2  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "\n",
      "\n",
      "Results for k=7:\n",
      "Accuracy: 0.9916666666666667\n",
      "Confusion Matrix:\n",
      "[[36  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 34  0  0  0  0  1  0  0]\n",
      " [ 0  0  0 37  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 36  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  2  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "\n",
      "\n",
      "Results for k=9:\n",
      "Accuracy: 0.9916666666666667\n",
      "Confusion Matrix:\n",
      "[[36  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 34  0  0  0  0  1  0  0]\n",
      " [ 0  0  0 37  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 36  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  2  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k  # Number of neighbors to consider\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X  # Training data\n",
    "        self.y_train = y  # Training labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        # For each example in X, predict its label\n",
    "        predicted_labels = [self._predict(x) for x in X]\n",
    "        return np.array(predicted_labels)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute the Euclidean distance from x to each example in the training set\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "\n",
    "        # Get the indices of the k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        # Get the labels of the k nearest neighbors\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        # Return the most common label among the k nearest neighbors\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Create feature and target arrays\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "# Trying out with different k values\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Results for k={k}:\")\n",
    "    # Initialize the KNN classifier\n",
    "    classifier = KNN(k=k)\n",
    "    # Fit the classifier to the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Predict the labels of the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Compute the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    # Compute the confusion matrix of the classifier\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d3efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 26s 13ms/step - loss: 0.1525 - accuracy: 0.9521\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0452 - accuracy: 0.9859\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0327 - accuracy: 0.9897\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0256 - accuracy: 0.9919\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0206 - accuracy: 0.9934\n",
      "313/313 - 1s - loss: 0.0339 - accuracy: 0.9900 - 1s/epoch - 4ms/step\n",
      "\n",
      "Test accuracy: 0.9900000095367432\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Confusion Matrix:\n",
      " [[ 977    0    0    0    0    0    1    2    0    0]\n",
      " [   0 1127    2    0    0    0    1    4    0    1]\n",
      " [   1    1 1021    0    1    0    0    8    0    0]\n",
      " [   0    0    1 1005    0    1    0    3    0    0]\n",
      " [   0    0    1    0  974    0    1    0    0    6]\n",
      " [   0    0    1   15    0  874    2    0    0    0]\n",
      " [   1    4    0    0    3    1  949    0    0    0]\n",
      " [   0    1    3    1    0    0    0 1020    0    3]\n",
      " [   4    0    3    0    0    2    0    3  957    5]\n",
      " [   0    0    0    0    3    3    1    6    0  996]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load and split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reshape the images\n",
    "train_images = train_images.reshape((-1, 28, 28, 1))\n",
    "test_images = test_images.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Create the convolutional base\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add Dense layers on top\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_pred = model.predict(test_images)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "confusion_mtx = confusion_matrix(test_labels, y_pred_classes) \n",
    "print('Confusion Matrix:\\n', confusion_mtx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
